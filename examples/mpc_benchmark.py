#!/usr/bin/env python3
"""
MPC ì»¨íŠ¸ë¡¤ëŸ¬ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ MPC ì»¨íŠ¸ë¡¤ëŸ¬ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ë¹„êµí•©ë‹ˆë‹¤.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ë²¤ì¹˜ë§ˆí¬ ì‹œë‚˜ë¦¬ì˜¤                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. ì›í˜• ê¶¤ì : ì¼ì •í•œ ê³¡ë¥ ì˜ ê²½ë¡œ ì¶”ì¢… ì„±ëŠ¥                           â”‚
â”‚ 2. Sì ê¶¤ì : ê¸‰ê²©í•œ ë°©í–¥ ì „í™˜ ì„±ëŠ¥                                   â”‚
â”‚ 3. ì§ì„  ê¶¤ì : ì§ì„  ì£¼í–‰ ë° ì†ë„ ì œì–´ ì„±ëŠ¥                            â”‚
â”‚ 4. íŒŒë¼ë¯¸í„° ë³€í™”: N, Q, R, dt ë“±ì˜ ì˜í–¥ ë¶„ì„                        â”‚
â”‚ 5. ì†Œí”„íŠ¸ ì œì•½ì¡°ê±´: ì œì•½ì¡°ê±´ ìœ ë¬´ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì¸¡ì • ì§€í‘œ:
- Solve Time: ìµœì í™” ë¬¸ì œ í’€ì´ ì‹œê°„
- Tracking Error: ê²½ë¡œ ì¶”ì¢… ì˜¤ì°¨ (ìœ„ì¹˜ ì˜¤ì°¨)
- Control Effort: ì œì–´ ì…ë ¥ í¬ê¸° (ì—ë„ˆì§€ íš¨ìœ¨)
- Smoothness: ì œì–´ ì…ë ¥ ë³€í™”ìœ¨ (ìŠ¹ì°¨ê°)
- Success Rate: ëª©í‘œ ë„ë‹¬ ì„±ê³µë¥ 
"""

import argparse
import time
from dataclasses import dataclass, field
from typing import Dict, List, Tuple
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec

from mpc_controller.controllers.mpc import MPCController, MPCParams
from mpc_controller.models.differential_drive import RobotParams
from mpc_controller.models.soft_constraints import SoftConstraintParams
from mpc_controller.utils import (
    generate_circle_trajectory,
    generate_figure_eight_trajectory,
    generate_sinusoidal_trajectory,
    normalize_angle,
)


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥ êµ¬ì¡°ì²´."""

    scenario_name: str
    config_name: str

    # ìƒíƒœ ë° ì œì–´ ì…ë ¥ íˆìŠ¤í† ë¦¬
    states: np.ndarray
    controls: np.ndarray
    reference: np.ndarray

    # ì„±ëŠ¥ ì§€í‘œ
    solve_times: np.ndarray
    tracking_errors: np.ndarray
    control_efforts: np.ndarray
    control_smoothness: np.ndarray

    # í†µê³„
    avg_solve_time: float = 0.0
    max_solve_time: float = 0.0
    std_solve_time: float = 0.0

    avg_tracking_error: float = 0.0
    max_tracking_error: float = 0.0
    std_tracking_error: float = 0.0

    total_control_effort: float = 0.0
    avg_control_smoothness: float = 0.0

    success: bool = True
    total_time: float = 0.0
    num_steps: int = 0

    def __post_init__(self):
        """í†µê³„ ê³„ì‚°."""
        if len(self.solve_times) > 0:
            self.avg_solve_time = float(np.mean(self.solve_times))
            self.max_solve_time = float(np.max(self.solve_times))
            self.std_solve_time = float(np.std(self.solve_times))

        if len(self.tracking_errors) > 0:
            self.avg_tracking_error = float(np.mean(self.tracking_errors))
            self.max_tracking_error = float(np.max(self.tracking_errors))
            self.std_tracking_error = float(np.std(self.tracking_errors))

        if len(self.control_efforts) > 0:
            self.total_control_effort = float(np.sum(self.control_efforts))

        if len(self.control_smoothness) > 0:
            self.avg_control_smoothness = float(np.mean(self.control_smoothness))

        self.num_steps = len(self.states) if self.states is not None else 0


@dataclass
class BenchmarkConfig:
    """ë²¤ì¹˜ë§ˆí¬ ì„¤ì •."""

    name: str
    mpc_params: MPCParams
    description: str = ""


def generate_benchmark_trajectories() -> Dict[str, np.ndarray]:
    """
    ë²¤ì¹˜ë§ˆí¬ìš© ë‹¤ì–‘í•œ ì°¸ì¡° ê¶¤ì  ìƒì„±.

    Returns:
        ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ì„ í‚¤ë¡œ í•˜ëŠ” ê¶¤ì  ë”•ì…”ë„ˆë¦¬
    """
    trajectories = {}

    # 1. ì›í˜• ê¶¤ì  (ì¼ì •í•œ ê³¡ë¥ )
    trajectories["circle"] = generate_circle_trajectory(
        center=np.array([0.0, 0.0]),
        radius=2.0,
        num_points=200
    )

    # 2. Sì ê¶¤ì  (ê¸‰ê²©í•œ ë°©í–¥ ì „í™˜)
    trajectories["figure8"] = generate_figure_eight_trajectory(
        center=np.array([0.0, 0.0]),
        scale=2.0,
        num_points=200
    )

    # 3. ì§ì„  ê¶¤ì  (ì†ë„ ì œì–´)
    t = np.linspace(0, 10, 200)
    x = t
    y = np.zeros_like(t)
    theta = np.zeros_like(t)
    trajectories["straight"] = np.column_stack([x, y, theta])

    # 4. ì •í˜„íŒŒ ê¶¤ì  (ë¶€ë“œëŸ¬ìš´ ê³¡ì„ )
    trajectories["sine"] = generate_sinusoidal_trajectory(
        start=np.array([0.0, 0.0]),
        length=10.0,
        amplitude=1.5,
        frequency=0.5,
        num_points=200
    )

    return trajectories


def create_benchmark_configs() -> List[BenchmarkConfig]:
    """
    ë²¤ì¹˜ë§ˆí¬ìš© ë‹¤ì–‘í•œ MPC ì„¤ì • ìƒì„±.

    Returns:
        BenchmarkConfig ë¦¬ìŠ¤íŠ¸
    """
    configs = []

    # 1. ê¸°ë³¸ ì„¤ì •
    configs.append(BenchmarkConfig(
        name="baseline",
        mpc_params=MPCParams(
            N=20,
            dt=0.1,
            Q=np.diag([10.0, 10.0, 1.0]),
            R=np.diag([0.1, 0.1]),
            Qf=np.diag([100.0, 100.0, 10.0]),
            Rd=np.diag([0.5, 0.5]),
        ),
        description="ê¸°ë³¸ MPC ì„¤ì • (N=20, dt=0.1)"
    ))

    # 2. ì§§ì€ ì˜ˆì¸¡ í˜¸ë¼ì´ì¦Œ
    configs.append(BenchmarkConfig(
        name="short_horizon",
        mpc_params=MPCParams(
            N=10,
            dt=0.1,
            Q=np.diag([10.0, 10.0, 1.0]),
            R=np.diag([0.1, 0.1]),
        ),
        description="ì§§ì€ ì˜ˆì¸¡ í˜¸ë¼ì´ì¦Œ (N=10)"
    ))

    # 3. ê¸´ ì˜ˆì¸¡ í˜¸ë¼ì´ì¦Œ
    configs.append(BenchmarkConfig(
        name="long_horizon",
        mpc_params=MPCParams(
            N=30,
            dt=0.1,
            Q=np.diag([10.0, 10.0, 1.0]),
            R=np.diag([0.1, 0.1]),
        ),
        description="ê¸´ ì˜ˆì¸¡ í˜¸ë¼ì´ì¦Œ (N=30)"
    ))

    # 4. ë†’ì€ ì œì–´ ê°€ì¤‘ì¹˜ (ë¶€ë“œëŸ¬ìš´ ì œì–´)
    configs.append(BenchmarkConfig(
        name="smooth_control",
        mpc_params=MPCParams(
            N=20,
            dt=0.1,
            Q=np.diag([10.0, 10.0, 1.0]),
            R=np.diag([1.0, 1.0]),  # ë†’ì€ R
            Rd=np.diag([2.0, 2.0]),  # ë†’ì€ Rd
        ),
        description="ë¶€ë“œëŸ¬ìš´ ì œì–´ (ë†’ì€ R, Rd)"
    ))

    # 5. ë†’ì€ ì¶”ì  ì •í™•ë„
    configs.append(BenchmarkConfig(
        name="high_accuracy",
        mpc_params=MPCParams(
            N=20,
            dt=0.1,
            Q=np.diag([50.0, 50.0, 5.0]),  # ë†’ì€ Q
            R=np.diag([0.05, 0.05]),  # ë‚®ì€ R
            Qf=np.diag([200.0, 200.0, 20.0]),
        ),
        description="ë†’ì€ ì¶”ì  ì •í™•ë„ (ë†’ì€ Q, ë‚®ì€ R)"
    ))

    # 6. ë¹ ë¥¸ ìƒ˜í”Œë§
    configs.append(BenchmarkConfig(
        name="fast_sampling",
        mpc_params=MPCParams(
            N=20,
            dt=0.05,  # ë¹ ë¥¸ ìƒ˜í”Œë§
            Q=np.diag([10.0, 10.0, 1.0]),
            R=np.diag([0.1, 0.1]),
        ),
        description="ë¹ ë¥¸ ìƒ˜í”Œë§ (dt=0.05)"
    ))

    # 7. ì†Œí”„íŠ¸ ì œì•½ì¡°ê±´ í™œì„±í™”
    configs.append(BenchmarkConfig(
        name="soft_constraints",
        mpc_params=MPCParams(
            N=20,
            dt=0.1,
            Q=np.diag([10.0, 10.0, 1.0]),
            R=np.diag([0.1, 0.1]),
            soft_constraints=SoftConstraintParams(
                enable_velocity_soft=True,
                enable_acceleration_soft=True,
                velocity_weight=100.0,
                acceleration_weight=100.0,
            ),
        ),
        description="ì†Œí”„íŠ¸ ì œì•½ì¡°ê±´ í™œì„±í™”"
    ))

    return configs


def simulate_mpc(
    controller: MPCController,
    reference: np.ndarray,
    initial_state: np.ndarray,
    scenario_name: str,
    config_name: str,
    dt: float = 0.05,
    max_steps: int = 500,
) -> BenchmarkResult:
    """
    MPC ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ë° ì„±ëŠ¥ ì¸¡ì •.

    Args:
        controller: MPC ì»¨íŠ¸ë¡¤ëŸ¬
        reference: ì°¸ì¡° ê¶¤ì  (N, 3)
        initial_state: ì´ˆê¸° ìƒíƒœ [x, y, theta]
        scenario_name: ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„
        config_name: ì„¤ì • ì´ë¦„
        dt: ì‹œë®¬ë ˆì´ì…˜ íƒ€ì„ìŠ¤í…
        max_steps: ìµœëŒ€ ìŠ¤í… ìˆ˜

    Returns:
        BenchmarkResult
    """
    # ìƒíƒœ ë° ì œì–´ ì…ë ¥ ì €ì¥
    states = [initial_state.copy()]
    controls = []
    solve_times = []
    tracking_errors = []
    control_efforts = []
    control_smoothness = []

    current_state = initial_state.copy()
    controller.reset()

    start_time = time.perf_counter()
    success = True

    for step in range(max_steps):
        # ê°€ì¥ ê°€ê¹Œìš´ ì°¸ì¡°ì  ì°¾ê¸°
        distances = np.sqrt(
            (reference[:, 0] - current_state[0])**2 +
            (reference[:, 1] - current_state[1])**2
        )
        closest_idx = np.argmin(distances)

        # MPC í˜¸ë¼ì´ì¦Œë§Œí¼ ì°¸ì¡° ê¶¤ì  ì¶”ì¶œ
        horizon = controller.N + 1
        ref_segment = np.zeros((horizon, 3))

        if closest_idx + horizon <= len(reference):
            ref_segment = reference[closest_idx:closest_idx + horizon]
        else:
            # ëì— ë„ë‹¬í•˜ë©´ ë§ˆì§€ë§‰ ì ìœ¼ë¡œ íŒ¨ë”©
            remaining = len(reference) - closest_idx
            ref_segment[:remaining] = reference[closest_idx:]
            ref_segment[remaining:] = reference[-1]

        # ì œì–´ ì…ë ¥ ê³„ì‚°
        try:
            control, info = controller.compute_control(current_state, ref_segment)
            solve_times.append(info['solve_time'])
        except Exception as e:
            print(f"  âš ï¸  ìµœì í™” ì‹¤íŒ¨: {e}")
            success = False
            break

        # ì¶”ì  ì˜¤ì°¨ ê³„ì‚° (ìœ„ì¹˜ ì˜¤ì°¨)
        error = np.sqrt(
            (current_state[0] - reference[closest_idx, 0])**2 +
            (current_state[1] - reference[closest_idx, 1])**2
        )
        tracking_errors.append(error)

        # ì œì–´ ë…¸ë ¥ ê³„ì‚° (v^2 + omega^2)
        effort = control[0]**2 + control[1]**2
        control_efforts.append(effort)

        # ì œì–´ í‰í™œë„ ê³„ì‚° (ì´ì „ ì œì–´ì™€ì˜ ì°¨ì´)
        if len(controls) > 0:
            smoothness = np.sqrt(
                (control[0] - controls[-1][0])**2 +
                (control[1] - controls[-1][1])**2
            )
            control_smoothness.append(smoothness)

        # ìƒíƒœ ì—…ë°ì´íŠ¸ (Euler integration)
        v, omega = control
        current_state[0] += v * np.cos(current_state[2]) * dt
        current_state[1] += v * np.sin(current_state[2]) * dt
        current_state[2] += omega * dt
        current_state[2] = normalize_angle(current_state[2])

        states.append(current_state.copy())
        controls.append(control)

        # ëª©í‘œ ë„ë‹¬ í™•ì¸
        goal_dist = np.sqrt(
            (current_state[0] - reference[-1, 0])**2 +
            (current_state[1] - reference[-1, 1])**2
        )

        if goal_dist < 0.1 and closest_idx > len(reference) - 10:
            break

    total_time = time.perf_counter() - start_time

    # BenchmarkResult ìƒì„±
    result = BenchmarkResult(
        scenario_name=scenario_name,
        config_name=config_name,
        states=np.array(states),
        controls=np.array(controls),
        reference=reference,
        solve_times=np.array(solve_times),
        tracking_errors=np.array(tracking_errors),
        control_efforts=np.array(control_efforts),
        control_smoothness=np.array(control_smoothness),
        success=success,
        total_time=total_time,
    )

    return result


def plot_benchmark_summary(
    results: List[BenchmarkResult],
    save_path: str = None
) -> None:
    """
    ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¢…í•© ì‹œê°í™”.

    Args:
        results: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ í™”ë©´ì—ë§Œ í‘œì‹œ)
    """
    if not results:
        print("âš ï¸  ì‹œê°í™”í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì‹œë‚˜ë¦¬ì˜¤ë³„ ê·¸ë£¹í™”
    scenarios = {}
    for result in results:
        if result.scenario_name not in scenarios:
            scenarios[result.scenario_name] = []
        scenarios[result.scenario_name].append(result)

    # ê° ì‹œë‚˜ë¦¬ì˜¤ë§ˆë‹¤ ê·¸ë˜í”„ ìƒì„±
    for scenario_name, scenario_results in scenarios.items():
        fig = plt.figure(figsize=(18, 12))
        gs = GridSpec(3, 4, figure=fig, hspace=0.3, wspace=0.3)

        # ìƒ‰ìƒ ë§µ
        colors = plt.cm.tab10(np.linspace(0, 1, len(scenario_results)))

        # 1. ê¶¤ì  ë¹„êµ
        ax1 = fig.add_subplot(gs[0, :2])
        ref = scenario_results[0].reference
        ax1.plot(ref[:, 0], ref[:, 1], 'k--', linewidth=2.5, label='Reference', alpha=0.7)

        for i, result in enumerate(scenario_results):
            ax1.plot(result.states[:, 0], result.states[:, 1],
                    color=colors[i], linewidth=1.5, label=result.config_name, alpha=0.8)

        ax1.scatter([ref[0, 0]], [ref[0, 1]], c='green', s=150, marker='o',
                   zorder=5, label='Start', edgecolors='black', linewidths=1.5)
        ax1.scatter([ref[-1, 0]], [ref[-1, 1]], c='red', s=200, marker='*',
                   zorder=5, label='Goal', edgecolors='black', linewidths=1.5)
        ax1.set_xlabel('X [m]', fontsize=11)
        ax1.set_ylabel('Y [m]', fontsize=11)
        ax1.set_title(f'{scenario_name.capitalize()} Trajectory Comparison', fontsize=12, fontweight='bold')
        ax1.legend(fontsize=9, loc='best')
        ax1.axis('equal')
        ax1.grid(True, alpha=0.3)

        # 2. Solve Time ë¹„êµ
        ax2 = fig.add_subplot(gs[0, 2])
        config_names = [r.config_name for r in scenario_results]
        solve_time_avg = [r.avg_solve_time * 1000 for r in scenario_results]
        solve_time_std = [r.std_solve_time * 1000 for r in scenario_results]

        bars = ax2.bar(range(len(config_names)), solve_time_avg,
                      yerr=solve_time_std, capsize=5, color=colors, alpha=0.7, edgecolor='black')
        ax2.set_xticks(range(len(config_names)))
        ax2.set_xticklabels(config_names, rotation=45, ha='right', fontsize=8)
        ax2.set_ylabel('Avg Solve Time [ms]', fontsize=10)
        ax2.set_title('Solve Time Comparison', fontsize=11, fontweight='bold')
        ax2.grid(True, alpha=0.3, axis='y')

        # ê°’ í‘œì‹œ
        for i, bar in enumerate(bars):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.2f}', ha='center', va='bottom', fontsize=8)

        # 3. Tracking Error ë¹„êµ
        ax3 = fig.add_subplot(gs[0, 3])
        error_avg = [r.avg_tracking_error for r in scenario_results]
        error_max = [r.max_tracking_error for r in scenario_results]

        x_pos = np.arange(len(config_names))
        width = 0.35
        bars1 = ax3.bar(x_pos - width/2, error_avg, width, label='Avg Error',
                       color=colors, alpha=0.7, edgecolor='black')
        bars2 = ax3.bar(x_pos + width/2, error_max, width, label='Max Error',
                       color=colors, alpha=0.5, edgecolor='black', hatch='//')

        ax3.set_xticks(x_pos)
        ax3.set_xticklabels(config_names, rotation=45, ha='right', fontsize=8)
        ax3.set_ylabel('Tracking Error [m]', fontsize=10)
        ax3.set_title('Tracking Error Comparison', fontsize=11, fontweight='bold')
        ax3.legend(fontsize=9)
        ax3.grid(True, alpha=0.3, axis='y')

        # 4. Tracking Error ì‹œê°„ ë³€í™”
        ax4 = fig.add_subplot(gs[1, :2])
        for i, result in enumerate(scenario_results):
            time_vec = np.arange(len(result.tracking_errors)) * 0.05
            ax4.plot(time_vec, result.tracking_errors,
                    color=colors[i], linewidth=1.5, label=result.config_name, alpha=0.8)

        ax4.set_xlabel('Time [s]', fontsize=11)
        ax4.set_ylabel('Tracking Error [m]', fontsize=11)
        ax4.set_title('Tracking Error over Time', fontsize=12, fontweight='bold')
        ax4.legend(fontsize=9, loc='best')
        ax4.grid(True, alpha=0.3)

        # 5. Solve Time ì‹œê°„ ë³€í™”
        ax5 = fig.add_subplot(gs[1, 2:])
        for i, result in enumerate(scenario_results):
            time_vec = np.arange(len(result.solve_times)) * 0.05
            ax5.plot(time_vec, result.solve_times * 1000,
                    color=colors[i], linewidth=1.5, label=result.config_name, alpha=0.8)

        ax5.set_xlabel('Time [s]', fontsize=11)
        ax5.set_ylabel('Solve Time [ms]', fontsize=11)
        ax5.set_title('Solve Time over Time', fontsize=12, fontweight='bold')
        ax5.legend(fontsize=9, loc='best')
        ax5.grid(True, alpha=0.3)

        # 6. Control Effort ë¹„êµ
        ax6 = fig.add_subplot(gs[2, 0])
        effort = [r.total_control_effort for r in scenario_results]
        bars = ax6.bar(range(len(config_names)), effort,
                      color=colors, alpha=0.7, edgecolor='black')
        ax6.set_xticks(range(len(config_names)))
        ax6.set_xticklabels(config_names, rotation=45, ha='right', fontsize=8)
        ax6.set_ylabel('Total Control Effort', fontsize=10)
        ax6.set_title('Control Effort Comparison', fontsize=11, fontweight='bold')
        ax6.grid(True, alpha=0.3, axis='y')

        # 7. Control Smoothness ë¹„êµ
        ax7 = fig.add_subplot(gs[2, 1])
        smoothness = [r.avg_control_smoothness for r in scenario_results]
        bars = ax7.bar(range(len(config_names)), smoothness,
                      color=colors, alpha=0.7, edgecolor='black')
        ax7.set_xticks(range(len(config_names)))
        ax7.set_xticklabels(config_names, rotation=45, ha='right', fontsize=8)
        ax7.set_ylabel('Avg Control Smoothness', fontsize=10)
        ax7.set_title('Control Smoothness Comparison', fontsize=11, fontweight='bold')
        ax7.grid(True, alpha=0.3, axis='y')

        # 8. ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸”
        ax8 = fig.add_subplot(gs[2, 2:])
        ax8.axis('off')

        # í…Œì´ë¸” ë°ì´í„° ì¤€ë¹„
        table_data = []
        headers = ['Config', 'Solve [ms]', 'Error [m]', 'Effort', 'Steps', 'Time [s]']

        for result in scenario_results:
            row = [
                result.config_name[:15],
                f'{result.avg_solve_time*1000:.2f}',
                f'{result.avg_tracking_error:.4f}',
                f'{result.total_control_effort:.1f}',
                f'{result.num_steps}',
                f'{result.total_time:.2f}'
            ]
            table_data.append(row)

        # í…Œì´ë¸” ìƒì„±
        table = ax8.table(cellText=table_data, colLabels=headers,
                         cellLoc='center', loc='center',
                         colWidths=[0.25, 0.15, 0.15, 0.15, 0.15, 0.15])
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)

        # í—¤ë” ìŠ¤íƒ€ì¼
        for i in range(len(headers)):
            table[(0, i)].set_facecolor('#4CAF50')
            table[(0, i)].set_text_props(weight='bold', color='white')

        # í–‰ ìƒ‰ìƒ êµëŒ€
        for i in range(1, len(table_data) + 1):
            for j in range(len(headers)):
                if i % 2 == 0:
                    table[(i, j)].set_facecolor('#f0f0f0')

        # ì „ì²´ ì œëª©
        fig.suptitle(f'MPC Benchmark: {scenario_name.capitalize()} Scenario',
                    fontsize=16, fontweight='bold', y=0.98)

        # ì €ì¥
        if save_path:
            scenario_save_path = save_path.replace('.png', f'_{scenario_name}.png')
            plt.savefig(scenario_save_path, dpi=150, bbox_inches='tight')
            print(f"âœ“ ê·¸ë˜í”„ ì €ì¥: {scenario_save_path}")

    plt.show()


def print_benchmark_table(results: List[BenchmarkResult]) -> None:
    """
    ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ í‘œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥.

    Args:
        results: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    if not results:
        print("âš ï¸  ì¶œë ¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("\n" + "="*120)
    print("                                      MPC CONTROLLER BENCHMARK RESULTS")
    print("="*120)

    # ì‹œë‚˜ë¦¬ì˜¤ë³„ ê·¸ë£¹í™”
    scenarios = {}
    for result in results:
        if result.scenario_name not in scenarios:
            scenarios[result.scenario_name] = []
        scenarios[result.scenario_name].append(result)

    # ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ ì¶œë ¥
    for scenario_name, scenario_results in scenarios.items():
        print(f"\nâ”Œâ”€ Scenario: {scenario_name.upper()} " + "â”€" * (105 - len(scenario_name)))
        print("â”‚")

        # í…Œì´ë¸” í—¤ë”
        header = (
            f"â”‚ {'Config':<20} â”‚ "
            f"{'Solve Time [ms]':<20} â”‚ "
            f"{'Tracking Error [m]':<25} â”‚ "
            f"{'Control':<15} â”‚ "
            f"{'Steps':<8} â”‚"
        )
        print(header)
        print("â”‚" + "â”€" * 117 + "â”‚")

        subheader = (
            f"â”‚ {'':<20} â”‚ "
            f"{'Avg':>8}  {'Max':>8}    â”‚ "
            f"{'Avg':>10}  {'Max':>10}    â”‚ "
            f"{'Effort':>15} â”‚ "
            f"{'':<8} â”‚"
        )
        print(subheader)
        print("â”‚" + "â”€" * 117 + "â”‚")

        # ê° ì„¤ì •ë³„ ê²°ê³¼ ì¶œë ¥
        for result in scenario_results:
            status = "âœ“" if result.success else "âœ—"
            row = (
                f"â”‚ {status} {result.config_name:<17} â”‚ "
                f"{result.avg_solve_time*1000:8.2f}  {result.max_solve_time*1000:8.2f}    â”‚ "
                f"{result.avg_tracking_error:10.4f}  {result.max_tracking_error:10.4f}    â”‚ "
                f"{result.total_control_effort:15.1f} â”‚ "
                f"{result.num_steps:8d} â”‚"
            )
            print(row)

        print("â””" + "â”€" * 117 + "â”˜")

        # ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ ë² ìŠ¤íŠ¸ ì„¤ì • ì°¾ê¸°
        best_solve = min(scenario_results, key=lambda r: r.avg_solve_time)
        best_error = min(scenario_results, key=lambda r: r.avg_tracking_error)
        best_effort = min(scenario_results, key=lambda r: r.total_control_effort)

        print(f"\n  ğŸ† Best Solve Time:      {best_solve.config_name:<20} ({best_solve.avg_solve_time*1000:.2f} ms)")
        print(f"  ğŸ¯ Best Tracking Error:  {best_error.config_name:<20} ({best_error.avg_tracking_error:.4f} m)")
        print(f"  âš¡ Best Control Effort:  {best_effort.config_name:<20} ({best_effort.total_control_effort:.1f})")

    print("\n" + "="*120 + "\n")


def run_benchmark(
    scenarios: List[str] = None,
    configs: List[str] = None,
    plot: bool = True,
    save_path: str = None,
) -> List[BenchmarkResult]:
    """
    ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰.

    Args:
        scenarios: ì‹¤í–‰í•  ì‹œë‚˜ë¦¬ì˜¤ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
        configs: ì‹¤í–‰í•  ì„¤ì • ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
        plot: ê·¸ë˜í”„ ì¶œë ¥ ì—¬ë¶€
        save_path: ê·¸ë˜í”„ ì €ì¥ ê²½ë¡œ

    Returns:
        ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    # ê¶¤ì  ë° ì„¤ì • ìƒì„±
    trajectories = generate_benchmark_trajectories()
    all_configs = create_benchmark_configs()

    # í•„í„°ë§
    if scenarios:
        trajectories = {k: v for k, v in trajectories.items() if k in scenarios}
    if configs:
        all_configs = [c for c in all_configs if c.name in configs]

    print("\n" + "="*80)
    print("                        MPC CONTROLLER BENCHMARK")
    print("="*80)
    print(f"\nì‹œë‚˜ë¦¬ì˜¤: {len(trajectories)}ê°œ")
    print(f"ì„¤ì •: {len(all_configs)}ê°œ")
    print(f"ì´ ì‹¤í–‰: {len(trajectories) * len(all_configs)}ê°œ\n")

    # ASCII í”Œë¡œìš°ì°¨íŠ¸
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚                         Benchmark Flow                              â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("â”‚  1. Generate Trajectories  â†’  [Circle, Figure8, Straight, Sine]    â”‚")
    print("â”‚  2. Create MPC Configs     â†’  [Baseline, Short/Long Horizon, ...]  â”‚")
    print("â”‚  3. Run Simulations        â†’  Measure Performance                   â”‚")
    print("â”‚  4. Collect Metrics        â†’  [Solve Time, Error, Effort, ...]     â”‚")
    print("â”‚  5. Analyze & Visualize    â†’  Tables + Graphs                       â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n")

    results = []
    total_runs = len(trajectories) * len(all_configs)
    current_run = 0

    # ëª¨ë“  ì¡°í•© ì‹¤í–‰
    for scenario_name, reference in trajectories.items():
        print(f"\nâ–¶ Scenario: {scenario_name.upper()}")
        print("â”€" * 80)

        initial_state = reference[0].copy()

        for config in all_configs:
            current_run += 1
            print(f"  [{current_run}/{total_runs}] {config.name:<20} ... ", end='', flush=True)

            # MPC ì»¨íŠ¸ë¡¤ëŸ¬ ìƒì„±
            controller = MPCController(
                mpc_params=config.mpc_params,
                enable_soft_constraints=(config.name == "soft_constraints"),
            )

            # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
            result = simulate_mpc(
                controller=controller,
                reference=reference,
                initial_state=initial_state.copy(),
                scenario_name=scenario_name,
                config_name=config.name,
                dt=0.05,
                max_steps=500,
            )

            results.append(result)

            # ê²°ê³¼ ì¶œë ¥
            status = "âœ“" if result.success else "âœ—"
            print(
                f"{status}  "
                f"Solve: {result.avg_solve_time*1000:6.2f}ms  "
                f"Error: {result.avg_tracking_error:7.4f}m  "
                f"Steps: {result.num_steps:4d}"
            )

    # ê²°ê³¼ ì¶œë ¥
    print_benchmark_table(results)

    # ì‹œê°í™”
    if plot:
        print("ğŸ“Š ê·¸ë˜í”„ ìƒì„± ì¤‘...\n")
        plot_benchmark_summary(results, save_path)

    return results


def main():
    """ë©”ì¸ í•¨ìˆ˜."""
    parser = argparse.ArgumentParser(
        description="MPC ì»¨íŠ¸ë¡¤ëŸ¬ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
  python mpc_benchmark.py

  # íŠ¹ì • ì‹œë‚˜ë¦¬ì˜¤ë§Œ ì‹¤í–‰
  python mpc_benchmark.py --scenarios circle figure8

  # íŠ¹ì • ì„¤ì •ë§Œ ì‹¤í–‰
  python mpc_benchmark.py --configs baseline soft_constraints

  # ê²°ê³¼ ê·¸ë˜í”„ ì €ì¥
  python mpc_benchmark.py --save benchmark_results.png

  # ê·¸ë˜í”„ ì—†ì´ í‘œë§Œ ì¶œë ¥
  python mpc_benchmark.py --no-plot
        """
    )

    parser.add_argument(
        '--scenarios',
        nargs='+',
        choices=['circle', 'figure8', 'straight', 'sine'],
        help='ì‹¤í–‰í•  ì‹œë‚˜ë¦¬ì˜¤ ì„ íƒ (ê¸°ë³¸: ì „ì²´)',
    )

    parser.add_argument(
        '--configs',
        nargs='+',
        choices=[
            'baseline', 'short_horizon', 'long_horizon',
            'smooth_control', 'high_accuracy', 'fast_sampling',
            'soft_constraints'
        ],
        help='ì‹¤í–‰í•  MPC ì„¤ì • ì„ íƒ (ê¸°ë³¸: ì „ì²´)',
    )

    parser.add_argument(
        '--save',
        type=str,
        default=None,
        help='ê²°ê³¼ ê·¸ë˜í”„ ì €ì¥ ê²½ë¡œ',
    )

    parser.add_argument(
        '--no-plot',
        action='store_true',
        help='ê·¸ë˜í”„ ì¶œë ¥ ìƒëµ (í‘œë§Œ ì¶œë ¥)',
    )

    args = parser.parse_args()

    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    results = run_benchmark(
        scenarios=args.scenarios,
        configs=args.configs,
        plot=not args.no_plot,
        save_path=args.save,
    )

    print("âœ“ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!\n")

    return results


if __name__ == "__main__":
    main()
